# –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è –ø–æ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è–º –¥–ª—è –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è ‚â•150 domains/hour

## 1. ‚úÖ Redis –ö–µ—à—É–≤–∞–Ω–Ω—è (HTML content, TTL: 1 hour)

### –û–ø–∏—Å
–ö–æ–∂–Ω–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –≤ Redis –Ω–∞ 1 –≥–æ–¥–∏–Ω—É. –ü–æ–≤—Ç–æ—Ä–Ω—ñ –∑–∞–ø–∏—Ç–∏ –¥–æ —Ç–æ–≥–æ –∂ –¥–æ–º–µ–Ω—É –æ—Ç—Ä–∏–º—É—é—Ç—å –¥–∞–Ω—ñ –∑ –∫–µ—à—É –º–∏—Ç—Ç—î–≤–æ.

### –Ü–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è
**–§–∞–π–ª:** `backend/app/core/cache.py`

```python
class RedisCache:
    def __init__(self):
        self.ttl = 3600  # 1 –≥–æ–¥–∏–Ω–∞
    
    async def get_html(self, domain: str) -> Optional[dict]:
        # –û—Ç—Ä–∏–º–∞—Ç–∏ HTML –∑ –∫–µ—à—É
        
    async def set_html(self, domain: str, html_data: dict):
        # –ó–±–µ—Ä–µ–≥—Ç–∏ HTML –Ω–∞ 1 –≥–æ–¥–∏–Ω—É
```

### –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ –∫–æ–¥—ñ

```python
# –£ scraper.py
result = await scraper.scrape_domain("example.com", use_cache=True)
if result['cached']:
    print("‚úì –û—Ç—Ä–∏–º–∞–Ω–æ –∑ –∫–µ—à—É (–º–∏—Ç—Ç—î–≤–æ)")
```

### API endpoints

```bash
# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à—É
GET /api/v1/cache/stats

# –û—á–∏—Å—Ç–∏—Ç–∏ –≤–µ—Å—å –∫–µ—à
DELETE /api/v1/cache/clear

# –í–∏–¥–∞–ª–∏—Ç–∏ –∫–µ—à –¥–ª—è –¥–æ–º–µ–Ω—É
DELETE /api/v1/cache/{domain}
```

### –í–∏–≥—Ä–∞—à –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
- **–ë–µ–∑ –∫–µ—à—É:** ~5-10 —Å–µ–∫—É–Ω–¥ –Ω–∞ –¥–æ–º–µ–Ω (–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è HTML)
- **–ó –∫–µ—à–µ–º:** ~0.01 —Å–µ–∫—É–Ω–¥–∞ (—á–∏—Ç–∞–Ω–Ω—è –∑ Redis)
- **–ü—Ä–∏—Ä—ñ—Å—Ç:** 500-1000x —à–≤–∏–¥—à–µ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤

---

## 2. ‚úÖ Rate Limiting (100 req/min –Ω–∞ IP)

### –û–ø–∏—Å
–û–±–º–µ–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç—ñ–≤ –¥–æ API –¥–ª—è –∑–∞—Ö–∏—Å—Ç—É –≤—ñ–¥ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ DDoS –∞—Ç–∞–∫.

### –Ü–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è
**–§–∞–π–ª:** `backend/app/core/rate_limiter.py`

```python
class RateLimiter:
    def __init__(self, max_requests: int = 100, window_seconds: int = 60):
        # 100 –∑–∞–ø–∏—Ç—ñ–≤ –Ω–∞ —Ö–≤–∏–ª–∏–Ω—É –Ω–∞ IP –∞–¥—Ä–µ—Å—É
```

### Response headers

```http
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 87
X-RateLimit-Reset: 45
```

### HTTP 429 Too Many Requests

```json
{
  "error": "Rate limit exceeded",
  "message": "Too many requests. Please try again in 45 seconds.",
  "retry_after": 45
}
```

### –í–∏–∫–ª—é—á–µ–Ω–Ω—è
- `/` - root endpoint
- `/api/v1/health` - health check

---

## 3. ‚úÖ Connection Pooling –¥–ª—è PostgreSQL

### –û–ø–∏—Å
–ü—É–ª –ø—ñ–¥–∫–ª—é—á–µ–Ω—å –¥–æ PostgreSQL –¥–ª—è —É—Å—É–Ω–µ–Ω–Ω—è overhead —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –∑'—î–¥–Ω–∞–Ω—å.

### –Ü–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è
**–§–∞–π–ª:** `backend/app/db/session.py`

```python
engine = create_engine(
    settings.DATABASE_URL,
    pool_pre_ping=True,      # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑'—î–¥–Ω–∞–Ω—å –ø–µ—Ä–µ–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º
    pool_size=10,            # 10 –ø–æ—Å—Ç—ñ–π–Ω–∏—Ö –∑'—î–¥–Ω–∞–Ω—å
    max_overflow=20          # +20 –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –ø—Ä–∏ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ
)
```

### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
- **pool_size:** 10 –ø–æ—Å—Ç—ñ–π–Ω–∏—Ö –∑'—î–¥–Ω–∞–Ω—å
- **max_overflow:** +20 —Ç–∏–º—á–∞—Å–æ–≤–∏—Ö (–≤—Å—å–æ–≥–æ –¥–æ 30)
- **pool_pre_ping:** –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∂–∏–≤–æ—Å—Ç—ñ –∑'—î–¥–Ω–∞–Ω–Ω—è

### –í–∏–≥—Ä–∞—à –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
- **–ë–µ–∑ pooling:** ~50-100ms –Ω–∞ –∑–∞–ø–∏—Ç (—Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑'—î–¥–Ω–∞–Ω–Ω—è)
- **–ó pooling:** ~1-5ms –Ω–∞ –∑–∞–ø–∏—Ç (–≥–æ—Ç–æ–≤–µ –∑'—î–¥–Ω–∞–Ω–Ω—è)
- **–ü—Ä–∏—Ä—ñ—Å—Ç:** 10-50x —à–≤–∏–¥—à–µ

---

## 4. ‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ HTTP –∑–∞–ø–∏—Ç–∏ (aiohttp)

### –û–ø–∏—Å
–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ HTTP –∫–ª—ñ—î–Ω—Ç–∞ –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–æ–∫.

### –Ü–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è
**–§–∞–π–ª:** `backend/app/services/scraper.py`

```python
class WebScraper:
    async def fetch_website(self, url: str):
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=self.timeout) as response:
                return await response.text()
```

### –í–∏–≥—Ä–∞—à –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
- **–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ –∑–∞–ø–∏—Ç–∏:** 1 –∑–∞–ø–∏—Ç –∑–∞ —Ä–∞–∑
- **–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ñ:** N –∑–∞–ø–∏—Ç—ñ–≤ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
- **–ó 10 workers:** 10 –¥–æ–º–µ–Ω—ñ–≤ –æ–¥–Ω–æ—á–∞—Å–Ω–æ

---

## 5. ‚úÖ 10 –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö Celery Workers

### –û–ø–∏—Å
Celery workers –æ–±—Ä–æ–±–ª—è—é—Ç—å –¥–æ–º–µ–Ω–∏ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ, –∫–æ–∂–µ–Ω –¥–æ–º–µ–Ω ‚Äì –æ–∫—Ä–µ–º–∏–π task.

### –Ü–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è
**–§–∞–π–ª:** `backend/app/tasks/celery_app.py`

```python
celery_app.conf.update(
    worker_concurrency=10,          # 10 –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö workers
    worker_prefetch_multiplier=1,   # –ö–æ–∂–µ–Ω worker –±–µ—Ä–µ –ø–æ 1 task
    worker_pool='solo'              # –î–ª—è Windows —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
)
```

**Docker Compose:**
```yaml
celery_worker:
  command: celery -A app.tasks.celery_app worker --loglevel=info --concurrency=10
```

### –í–∏–≥—Ä–∞—à –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
- **1 worker:** ~20 domains/hour
- **10 workers:** ~200-400 domains/hour (–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ)

---

## 6. ‚úÖ Proxy –†–æ—Ç–∞—Ü—ñ—è –∑ Retry Logic

### –û–ø–∏—Å
–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ —Ä–æ—Ç–∞—Ü—ñ—è –ø—Ä–æ–∫—Å—ñ –ø—Ä–∏ –ø–æ–º–∏–ª–∫–∞—Ö, –º–∞–∫—Å–∏–º—É–º 3 —Å–ø—Ä–æ–±–∏ –Ω–∞ –¥–æ–º–µ–Ω.

### –Ü–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è
**–§–∞–π–ª:** `backend/app/services/scraper.py`

```python
for attempt in range(self.max_retries):  # 3 —Å–ø—Ä–æ–±–∏
    try:
        proxy_url = self.proxy_rotator.get_next_proxy()
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ—Ä–µ–∑ proxy...
    except:
        # –ü–æ–∑–Ω–∞—á–∏—Ç–∏ proxy —è–∫ –Ω–µ–≤–¥–∞–ª–∏–π
        self.proxy_rotator.mark_proxy_failed(proxy_url)
        # Exponential backoff: 1s, 2s, 4s
        await asyncio.sleep(2 ** attempt)
```

### –í–∏–≥—Ä–∞—à –Ω–∞–¥—ñ–π–Ω–æ—Å—Ç—ñ
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ proxy
- –ü–µ—Ä–µ–º–∏–∫–∞–Ω–Ω—è –Ω–∞ —ñ–Ω—à–∏–π proxy
- Exponential backoff –º—ñ–∂ —Å–ø—Ä–æ–±–∞–º–∏

---

## 7. ‚úÖ –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è HTML –¥–ª—è Gemini

### –û–ø–∏—Å
–í–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Ç–µ–≥—ñ–≤ —Ç–∞ –æ–±–º–µ–∂–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É HTML –ø–µ—Ä–µ–¥ –≤—ñ–¥–ø—Ä–∞–≤–∫–æ—é –≤ Gemini.

### –Ü–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è
**–§–∞–π–ª:** `backend/app/services/scraper.py`

```python
def extract_visible_content(self, html: str):
    soup = BeautifulSoup(html, 'lxml')
    
    # –í–∏–¥–∞–ª—è—î–º–æ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ç–µ–≥–∏
    for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
        tag.decompose()
    
    # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä
    return {
        'text': text[:50000],           # 50KB —Ç–µ–∫—Å—Ç—É
        'clean_html': clean_html[:100000]  # 100KB HTML
    }
```

### –í–∏–≥—Ä–∞—à
- **–ü–æ–≤–Ω–∏–π HTML:** ~500KB-5MB ‚Üí –ø–æ–≤—ñ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ Gemini
- **–û—á–∏—â–µ–Ω–∏–π HTML:** ~50-100KB ‚Üí —à–≤–∏–¥–∫–∞ –æ–±—Ä–æ–±–∫–∞
- **–ü—Ä–∏—Ä—ñ—Å—Ç:** 5-10x —à–≤–∏–¥—à–µ –æ–±—Ä–æ–±–∫–∞ –≤ Gemini

---

## –ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ

### –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —à–≤–∏–¥–∫–æ—Å—Ç—ñ

**–ß–∞—Å –Ω–∞ 1 –¥–æ–º–µ–Ω (–∑ –∫–µ—à–µ–º):**
- Scraping: ~5 —Å–µ–∫—É–Ω–¥ (–∞–±–æ 0.01—Å –∑ –∫–µ—à—É)
- Gemini AI: ~3 —Å–µ–∫—É–Ω–¥–∏
- Webhook: ~1 —Å–µ–∫—É–Ω–¥–∞
- **–í—Å—å–æ–≥–æ:** ~9 —Å–µ–∫—É–Ω–¥ (–∞–±–æ ~4—Å –∑ –∫–µ—à—É)

**–ó 10 Celery workers:**
- Domains/second: 10 / 9 = ~1.1 domains/sec
- Domains/hour: 1.1 * 3600 = **~400 domains/hour**

**–ó –∫–µ—à—É–≤–∞–Ω–Ω—è–º (50% hit rate):**
- Domains/hour: **~500-600 domains/hour**

### –†–µ–∑—É–ª—å—Ç–∞—Ç
‚úÖ **400-600 domains/hour >> 150 domains/hour (–≤–∏–º–æ–≥–∞)**

---

## –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ

### API endpoints –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏

```bash
# Cache stats
curl http://localhost:8000/api/v1/cache/stats

# Rate limiter stats  
curl http://localhost:8000/api/v1/parsing/status

# Database pool info
curl http://localhost:8000/api/v1/health
```

### –õ–æ–≥–∏

```python
logger.info(f"‚úì –ö–µ—à HIT: {domain}")  # –ó –∫–µ—à—É
logger.info(f"‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑–∞ {time}—Å")  # Scraping
logger.info(f"‚úì Gemini: {len(deals)} —É–≥–æ–¥")  # Gemini
```

---

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è production

1. **Redis Persistence**
   ```yaml
   redis:
     command: redis-server --appendonly yes
   ```

2. **PostgreSQL max_connections**
   ```
   max_connections = 100
   shared_buffers = 256MB
   ```

3. **Nginx caching**
   ```nginx
   proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=my_cache:10m;
   ```

4. **Celery autoscaling**
   ```bash
   celery worker --autoscale=20,5  # 5-20 workers –¥–∏–Ω–∞–º—ñ—á–Ω–æ
   ```

5. **Monitoring**
   - Prometheus + Grafana –¥–ª—è –º–µ—Ç—Ä–∏–∫
   - Sentry –¥–ª—è –ø–æ–º–∏–ª–æ–∫
   - Redis Commander –¥–ª—è –∫–µ—à—É

---

## Troubleshooting

### –ü–æ–≤—ñ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞

1. –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ Redis –∫–µ—à:
   ```bash
   curl http://localhost:8000/api/v1/cache/stats
   ```

2. –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ Celery workers:
   ```bash
   docker-compose logs -f celery_worker
   ```

3. –û—á–∏—Å—Ç–∏—Ç–∏ –∫–µ—à:
   ```bash
   curl -X DELETE http://localhost:8000/api/v1/cache/clear
   ```

### Rate limit errors

1. –ó–±—ñ–ª—å—à–∏—Ç–∏ –ª—ñ–º—ñ—Ç –≤ `rate_limiter.py`:
   ```python
   rate_limiter = RateLimiter(max_requests=200, window_seconds=60)
   ```

### Out of memory

1. –ó–º–µ–Ω—à–∏—Ç–∏ TTL –∫–µ—à—É:
   ```python
   self.ttl = 1800  # 30 —Ö–≤–∏–ª–∏–Ω –∑–∞–º—ñ—Å—Ç—å 1 –≥–æ–¥–∏–Ω–∏
   ```

2. –ó–±—ñ–ª—å—à–∏—Ç–∏ maxmemory Redis:
   ```yaml
   redis:
     command: redis-server --maxmemory 2gb --maxmemory-policy allkeys-lru
   ```

---

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –í–°–Ü –û–ü–¢–ò–ú–Ü–ó–ê–¶–Ü–á –†–ï–ê–õ–Ü–ó–û–í–ê–ù–Ü

–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–æ –æ–±—Ä–æ–±–∫–∏ ‚â•150 domains/hour –∑ –∑–∞–ø–∞—Å–æ–º 2-3x! üöÄ
