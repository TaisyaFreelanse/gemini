import asyncio
import logging
from typing import Dict, List, Optional
from celery import Task
from app.tasks.celery_app import celery_app
from app.services.scraper import WebScraper
from app.services.gemini import GeminiService
from app.services.proxy import ProxyRotator
import redis
import json
from datetime import datetime

logger = logging.getLogger(__name__)

# Redis –∫–ª—ñ—î–Ω—Ç –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É
from app.core.config import settings
redis_client = redis.from_url(settings.REDIS_URL)


def _add_ui_log(level: str, message: str, domain: str = None, extra: dict = None):
    """–î–æ–¥–∞—Ç–∏ –ª–æ–≥ –¥–ª—è UI (–≤ Redis)"""
    try:
        from app.api.endpoints.logs import add_log
        add_log(level, message, domain, extra)
    except Exception:
        pass  # –ù–µ –±–ª–æ–∫—É–≤–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω–∏–π –ø—Ä–æ—Ü–µ—Å


class CallbackTask(Task):
    """–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è –∑–∞–¥–∞—á –∑ callback"""
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """–í–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ –∑–∞–¥–∞—á—ñ"""
        logger.error(f"–ó–∞–¥–∞—á–∞ {task_id} –Ω–µ –≤–∏–∫–æ–Ω–∞–Ω–∞: {exc}")
        # –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –ª–æ–≥—ñ–∫—É –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –≤ webhook –ø—Ä–æ –ø–æ–º–∏–ª–∫—É


@celery_app.task(bind=True, base=CallbackTask, name='scrape_domain_task')
def scrape_domain_task(self, domain: str, session_id: int, config: Optional[Dict] = None) -> Dict:
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω—É
    
    Args:
        domain: –î–æ–º–µ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É
        session_id: ID —Å–µ—Å—ñ—ó –ø–∞—Ä—Å–∏–Ω–≥—É
        config: –î–æ–¥–∞—Ç–∫–æ–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è (proxy, gemini key —Ç–æ—â–æ)
    
    Returns:
        Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥—É
    """
    task_id = self.request.id
    logger.info(f"[Task {task_id}] –ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É –¥–æ–º–µ–Ω—É: {domain}")
    _add_ui_log("INFO", f"–ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É –¥–æ–º–µ–Ω—É: {domain}", domain)
    
    # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å –≤ Redis
    _update_task_status(task_id, domain, "running", session_id)
    
    try:
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É –æ–±—Ä–æ–±–∫—É
        result = asyncio.run(_scrape_domain_async(domain, session_id, config or {}))
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å
        _update_task_status(task_id, domain, "completed", session_id, result)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å–µ—Å—ñ—é –≤ –ë–î
        _update_session_in_db(session_id, result)
        
        deals_count = result.get('deals_count', 0)
        if result.get('success'):
            logger.info(f"[Task {task_id}] ‚úì –ó–∞–≤–µ—Ä—à–µ–Ω–æ –ø–∞—Ä—Å–∏–Ω–≥ {domain}: {deals_count} —É–≥–æ–¥")
            _add_ui_log("INFO", f"‚úì –ó–∞–≤–µ—Ä—à–µ–Ω–æ –ø–∞—Ä—Å–∏–Ω–≥ {domain}: {deals_count} —É–≥–æ–¥", domain, {"deals_count": deals_count})
        else:
            error_msg = result.get('error', 'Unknown error')
            logger.warning(f"[Task {task_id}] ‚ö† –ü–∞—Ä—Å–∏–Ω–≥ {domain} –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑ –ø–æ–º–∏–ª–∫–æ—é: {error_msg}")
            _add_ui_log("WARNING", f"‚ö† –ü–∞—Ä—Å–∏–Ω–≥ {domain}: {error_msg[:100]}", domain)
        
        return result
    
    except Exception as e:
        logger.error(f"[Task {task_id}] ‚úó –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {domain}: {str(e)}", exc_info=True)
        _add_ui_log("ERROR", f"‚úó –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {domain}: {str(e)[:100]}", domain)
        
        error_result = {
            "success": False,
            "domain": domain,
            "error": str(e),
            "deals_count": 0
        }
        
        _update_task_status(task_id, domain, "failed", session_id, error_result)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å–µ—Å—ñ—é –≤ –ë–î
        _update_session_in_db(session_id, error_result)
        
        return error_result


async def _scrape_domain_async(domain: str, session_id: int, config: Dict) -> Dict:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –¥–æ–º–µ–Ω—É
    
    –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª: WebScraper ‚Üí GeminiService ‚Üí –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    """
    result = {
        "success": False,
        "domain": domain,
        "session_id": session_id,
        "deals_count": 0,
        "deals": [],
        "error": None,
        "scraped_at": datetime.utcnow().isoformat(),
        "metadata": {}
    }
    
    # –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ HTML —á–µ—Ä–µ–∑ WebScraper
    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ scraper –∑ –ø—Ä–æ–∫—Å—ñ —è–∫—â–æ —î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
        proxy_config = config.get('proxy')
        scraper = WebScraper.create_with_config(proxy_config) if proxy_config else WebScraper()
        
        logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è HTML –¥–ª—è {domain}...")
        _add_ui_log("DEBUG", f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è HTML –¥–ª—è {domain}...", domain)
        
        # use_cache=False ‚Äî async Redis –∫–µ—à –¥–∞—î "Event loop is closed" —É Celery
        scraped_data = await scraper.scrape_domain(domain, use_proxy=bool(proxy_config), use_cache=False)
        
        if not scraped_data['success']:
            error_msg = scraped_data.get('error', 'Scraping failed')
            result['error'] = error_msg
            _add_ui_log("ERROR", f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {domain}: {error_msg[:100]}", domain)
            return result
        
        html_len = len(scraped_data.get('html_raw', ''))
        result['metadata']['html_length'] = html_len
        _add_ui_log("INFO", f"‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ HTML –¥–ª—è {domain} ({html_len} –±–∞–π—Ç)", domain, {"html_length": html_len})
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ WebScraper –¥–ª—è {domain}: {e}")
        _add_ui_log("ERROR", f"WebScraper –ø–æ–º–∏–ª–∫–∞ –¥–ª—è {domain}: {str(e)[:100]}", domain)
        result['error'] = f"WebScraper error: {str(e)}"
        return result
    
    # –ö—Ä–æ–∫ 2: –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —á–µ—Ä–µ–∑ Gemini AI
    try:
        gemini_key = config.get('gemini_key')
        prompt_template = config.get('prompt')
        gemini = GeminiService(
            api_key=gemini_key or None,
            prompt_template=prompt_template
        )
        
        logger.info(f"–ê–Ω–∞–ª—ñ–∑ —á–µ—Ä–µ–∑ Gemini AI –¥–ª—è {domain}...")
        _add_ui_log("DEBUG", f"–ê–Ω–∞–ª—ñ–∑ —á–µ—Ä–µ–∑ Gemini AI –¥–ª—è {domain}...", domain)
        
        deals, error, metadata = await gemini.extract_deals_from_scraped_data(scraped_data)
        
        if error:
            result['error'] = error
            result['metadata']['gemini'] = metadata
            _add_ui_log("WARNING", f"Gemini –ø–æ–º–∏–ª–∫–∞ –¥–ª—è {domain}: {error[:100]}", domain)
            return result
        
        result['success'] = True
        result['deals_count'] = len(deals)
        result['deals'] = [deal.dict() for deal in deals]
        result['metadata']['gemini'] = metadata
        
        logger.info(f"‚úì –ó–Ω–∞–π–¥–µ–Ω–æ {len(deals)} —É–≥–æ–¥ –¥–ª—è {domain}")
        _add_ui_log("INFO", f"‚úì Gemini –∑–Ω–∞–π—à–æ–≤ {len(deals)} —É–≥–æ–¥ –¥–ª—è {domain}", domain, {"deals_count": len(deals)})
        
    except Exception as e:
        err_s = str(e).strip()
        if '"shop"' in err_s or err_s in ('\n    "shop"', '"shop"', "'shop'"):
            msg = "Gemini: –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø–æ—Ä–æ–∂–Ω—è –∞–±–æ –∑–∞–±–ª–æ–∫–æ–≤–∞–Ω–∞ (–Ω–µ–º–∞—î —Ç–µ–∫—Å—Ç—É –≤ parts)"
        else:
            msg = f"Gemini error: {err_s[:200]}"
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ Gemini –¥–ª—è {domain}: {msg}")
        _add_ui_log("ERROR", f"Gemini –ø–æ–º–∏–ª–∫–∞ –¥–ª—è {domain}: {msg[:100]}", domain)
        result['error'] = msg
        return result
    
    # –ö—Ä–æ–∫ 3: –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ë–î —Ç–∞ Redis
    try:
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ Redis –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É
        _save_scraping_result(session_id, domain, result)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –ë–î –¥–ª—è –ø–æ—Å—Ç—ñ–π–Ω–æ–≥–æ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è
        if result['success'] and result['deals_count'] > 0:
            from app.db.session import SessionLocal
            from app.db import crud
            
            db = SessionLocal()
            try:
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–∂–Ω—É —É–≥–æ–¥—É –≤ –ë–î
                for deal_data in result['deals']:
                    crud.create_scraped_deal(
                        db=db,
                        session_id=session_id,
                        domain=domain,
                        deal_data=deal_data
                    )
                logger.info(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ {result['deals_count']} —É–≥–æ–¥ –≤ –ë–î –¥–ª—è {domain}")
            except Exception as db_error:
                logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –ë–î: {db_error}")
            finally:
                db.close()
    except Exception as e:
        logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {e}")
    
    # –ö—Ä–æ–∫ 4: –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ webhook
    if result['success'] and result['deals_count'] > 0:
        try:
            from app.services.webhook import WebhookService
            
            webhook_config = config.get('webhook', {})
            webhook = WebhookService.create_from_config(webhook_config)
            
            logger.info(f"–í—ñ–¥–ø—Ä–∞–≤–∫–∞ {result['deals_count']} —É–≥–æ–¥ –≤ webhook...")
            _add_ui_log("DEBUG", f"–í—ñ–¥–ø—Ä–∞–≤–∫–∞ {result['deals_count']} —É–≥–æ–¥ –≤ webhook –¥–ª—è {domain}...", domain)
            
            webhook_result = await webhook.send_deals_from_scraping_result(result, session_id)
            
            result['webhook_sent'] = webhook_result['successful'] > 0
            result['webhook_stats'] = webhook_result
            
            successful = webhook_result['successful']
            total = webhook_result['total']
            logger.info(f"Webhook: {successful}/{total} —É—Å–ø—ñ—à–Ω–∏—Ö")
            
            if successful > 0:
                _add_ui_log("INFO", f"‚úì Webhook: –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ {successful}/{total} —É–≥–æ–¥ –¥–ª—è {domain}", domain, {"successful": successful, "total": total})
            else:
                _add_ui_log("WARNING", f"‚ö† Webhook: 0/{total} —É–≥–æ–¥ –¥–ª—è {domain}", domain)
                
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –≤ webhook: {e}")
            _add_ui_log("ERROR", f"Webhook –ø–æ–º–∏–ª–∫–∞ –¥–ª—è {domain}: {str(e)[:100]}", domain)
            result['webhook_sent'] = False
            result['webhook_error'] = str(e)
    
    return result


def _update_task_status(task_id: str, domain: str, status: str, session_id: int, result: Optional[Dict] = None):
    """–û–Ω–æ–≤–∏—Ç–∏ —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á—ñ –≤ Redis"""
    try:
        key = f"task:{task_id}"
        data = {
            "task_id": task_id,
            "domain": domain,
            "status": status,
            "session_id": session_id,
            "updated_at": datetime.utcnow().isoformat()
        }
        
        if result:
            data['result'] = result
        
        redis_client.setex(key, 3600, json.dumps(data))  # TTL 1 –≥–æ–¥–∏–Ω–∞
        
        # –¢–∞–∫–æ–∂ –æ–Ω–æ–≤–ª—é—î–º–æ –∑–∞–≥–∞–ª—å–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó
        _update_session_progress(session_id, status, domain)
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É –∑–∞–¥–∞—á—ñ: {e}")


def _update_session_progress(session_id: int, status: str, domain: str):
    """–û–Ω–æ–≤–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó –ø–∞—Ä—Å–∏–Ω–≥—É"""
    try:
        key = f"session:{session_id}:progress"
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å
        progress_data = redis_client.get(key)
        if progress_data:
            progress = json.loads(progress_data)
        else:
            progress = {
                "session_id": session_id,
                "total": 0,
                "processed": 0,
                "successful": 0,
                "failed": 0,
                "running": 0,
                "domains": {}
            }
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å –¥–æ–º–µ–Ω—É
        old_status = progress['domains'].get(domain)
        progress['domains'][domain] = status
        
        # –û–Ω–æ–≤–ª—é—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫–∏
        if old_status:
            if old_status == "running":
                progress['running'] -= 1
        
        if status == "running":
            progress['running'] += 1
        elif status == "completed":
            progress['processed'] += 1
            progress['successful'] += 1
        elif status == "failed":
            progress['processed'] += 1
            progress['failed'] += 1
        
        progress['updated_at'] = datetime.utcnow().isoformat()
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞–∑–∞–¥
        redis_client.setex(key, 7200, json.dumps(progress))  # TTL 2 –≥–æ–¥–∏–Ω–∏
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É —Å–µ—Å—ñ—ó: {e}")


def _save_scraping_result(session_id: int, domain: str, result: Dict):
    """–ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥—É –≤ Redis"""
    try:
        key = f"session:{session_id}:results:{domain}"
        redis_client.setex(key, 7200, json.dumps(result))  # TTL 2 –≥–æ–¥–∏–Ω–∏
        
        # –î–æ–¥–∞—î–º–æ –¥–æ–º–µ–Ω –¥–æ —Å–ø–∏—Å–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Å–µ—Å—ñ—ó
        list_key = f"session:{session_id}:domains"
        redis_client.sadd(list_key, domain)
        redis_client.expire(list_key, 7200)
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É: {e}")


def _update_session_in_db(session_id: int, result: Dict):
    """–û–Ω–æ–≤–∏—Ç–∏ —Å–µ—Å—ñ—é –ø–∞—Ä—Å–∏–Ω–≥—É –≤ –ë–î"""
    try:
        from app.db.session import SessionLocal
        from app.db import crud
        
        db = SessionLocal()
        try:
            session = crud.get_scraping_session(db, session_id)
            if session:
                # –û—Ç—Ä–∏–º—É—î–º–æ –ø–æ—Ç–æ—á–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
                processed = session.processed_domains or 0
                successful = session.successful_domains or 0
                failed = session.failed_domains or 0
                
                # –û–Ω–æ–≤–ª—é—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫–∏
                processed += 1
                if result.get('success'):
                    successful += 1
                else:
                    failed += 1
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –≤—Å—ñ –¥–æ–º–µ–Ω–∏ –æ–±—Ä–æ–±–ª–µ–Ω—ñ
                status = session.status
                if processed >= session.total_domains:
                    status = "completed"
                
                # –û–Ω–æ–≤–ª—é—î–º–æ —Å–µ—Å—ñ—é
                crud.update_scraping_session(
                    db=db,
                    session_id=session_id,
                    processed=processed,
                    successful=successful,
                    failed=failed,
                    status=status
                )
                
                # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å –≤ Redis
                if status == "completed":
                    redis_client.set("scraping:status", "completed")
                elif status == "failed":
                    redis_client.set("scraping:status", "failed")
                
                logger.debug(f"–û–Ω–æ–≤–ª–µ–Ω–æ —Å–µ—Å—ñ—é {session_id}: processed={processed}, successful={successful}, failed={failed}")
        finally:
            db.close()
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–µ—Å—ñ—ó –≤ –ë–î: {e}")


@celery_app.task(name='start_batch_scraping')
def start_batch_scraping(domains: List[str], session_id: int, config: Optional[Dict] = None) -> Dict:
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–∞–∫–µ—Ç–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–º–µ–Ω—ñ–≤
    
    Args:
        domains: –°–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω—ñ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É
        session_id: ID —Å–µ—Å—ñ—ó –ø–∞—Ä—Å–∏–Ω–≥—É
        config: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è –≤—Å—ñ—Ö –∑–∞–¥–∞—á
    
    Returns:
        Dict –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –∑–∞–ø—É—â–µ–Ω—ñ –∑–∞–¥–∞—á—ñ
    """
    logger.info(f"–ó–∞–ø—É—Å–∫ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥—É: {len(domains)} –¥–æ–º–µ–Ω—ñ–≤, —Å–µ—Å—ñ—è {session_id}")
    
    # –õ–æ–≥—É—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é (–±–µ–∑ –ø–∞—Ä–æ–ª—ñ–≤)
    proxy_info = "–ë–µ–∑ –ø—Ä–æ–∫—Å—ñ"
    if config and config.get('proxy') and config['proxy'].get('host'):
        proxy_info = f"–ü—Ä–æ–∫—Å—ñ: {config['proxy']['host']}:{config['proxy'].get('http_port', 59100)}"
    
    _add_ui_log("INFO", f"üöÄ –°—Ç–∞—Ä—Ç –ø–∞—Ä—Å–∏–Ω–≥—É: {len(domains)} –¥–æ–º–µ–Ω—ñ–≤, —Å–µ—Å—ñ—è #{session_id}", extra={
        "session_id": session_id,
        "total_domains": len(domains),
        "proxy": proxy_info
    })
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó
    _init_session_progress(session_id, domains)
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ –∑–∞–¥–∞—á—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–æ–º–µ–Ω—É
    task_ids = []
    for domain in domains:
        task = scrape_domain_task.delay(domain, session_id, config)
        task_ids.append({
            "task_id": task.id,
            "domain": domain
        })
    
    logger.info(f"–ó–∞–ø—É—â–µ–Ω–æ {len(task_ids)} –∑–∞–¥–∞—á –¥–ª—è —Å–µ—Å—ñ—ó {session_id}")
    _add_ui_log("INFO", f"üìã –ó–∞–ø—É—â–µ–Ω–æ {len(task_ids)} –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–æ–±–∫–∏", extra={"task_count": len(task_ids)})
    
    return {
        "session_id": session_id,
        "total_domains": len(domains),
        "task_ids": task_ids,
        "started_at": datetime.utcnow().isoformat()
    }


def _init_session_progress(session_id: int, domains: List[str]):
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó"""
    try:
        key = f"session:{session_id}:progress"
        progress = {
            "session_id": session_id,
            "total": len(domains),
            "processed": 0,
            "successful": 0,
            "failed": 0,
            "running": 0,
            "domains": {domain: "pending" for domain in domains},
            "started_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat()
        }
        redis_client.setex(key, 7200, json.dumps(progress))
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å—Ç–∞—Ç—É—Å —Å–µ—Å—ñ—ó
        redis_client.set(f"scraping:status", "running")
        redis_client.set(f"scraping:session_id", session_id)
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ø—Ä–æ–≥—Ä–µ—Å—É: {e}")


@celery_app.task(name='get_session_progress')
def get_session_progress(session_id: int) -> Optional[Dict]:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó –ø–∞—Ä—Å–∏–Ω–≥—É
    
    Args:
        session_id: ID —Å–µ—Å—ñ—ó
    
    Returns:
        Dict –∑ –ø—Ä–æ–≥—Ä–µ—Å–æ–º –∞–±–æ None
    """
    try:
        key = f"session:{session_id}:progress"
        data = redis_client.get(key)
        if data:
            return json.loads(data)
        return None
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É: {e}")
        return None


@celery_app.task(name='cleanup_old_sessions')
def cleanup_old_sessions():
    """
    –û—á–∏—Å—Ç–∏—Ç–∏ —Å—Ç–∞—Ä—ñ –¥–∞–Ω—ñ —Å–µ—Å—ñ–π –∑ Redis
    
    –ü–µ—Ä—ñ–æ–¥–∏—á–Ω–∞ –∑–∞–¥–∞—á–∞ –¥–ª—è Celery Beat
    """
    logger.info("–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö —Å–µ—Å—ñ–π...")
    # TODO: –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ª–æ–≥—ñ–∫—É –æ—á–∏—â–µ–Ω–Ω—è
    pass
