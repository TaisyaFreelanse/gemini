import asyncio
import logging
from typing import Dict, List, Optional
from celery import Task
from app.tasks.celery_app import celery_app
from app.services.scraper import WebScraper
from app.services.gemini import GeminiService
from app.services.proxy import ProxyRotator
import redis
import json
from datetime import datetime

logger = logging.getLogger(__name__)

# Redis –∫–ª—ñ—î–Ω—Ç –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É
from app.core.config import settings
redis_client = redis.from_url(settings.REDIS_URL)


def _add_ui_log(level: str, message: str, domain: str = None, extra: dict = None):
    """–î–æ–¥–∞—Ç–∏ –ª–æ–≥ –¥–ª—è UI (–≤ Redis)"""
    try:
        from app.api.endpoints.logs import add_log
        add_log(level, message, domain, extra)
    except Exception:
        pass  # –ù–µ –±–ª–æ–∫—É–≤–∞—Ç–∏ –æ—Å–Ω–æ–≤–Ω–∏–π –ø—Ä–æ—Ü–µ—Å


class CallbackTask(Task):
    """–ë–∞–∑–æ–≤–∏–π –∫–ª–∞—Å –¥–ª—è –∑–∞–¥–∞—á –∑ callback"""
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """–í–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ –∑–∞–¥–∞—á—ñ"""
        logger.error(f"–ó–∞–¥–∞—á–∞ {task_id} –Ω–µ –≤–∏–∫–æ–Ω–∞–Ω–∞: {exc}")
        # –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –ª–æ–≥—ñ–∫—É –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –≤ webhook –ø—Ä–æ –ø–æ–º–∏–ª–∫—É


def _is_stop_requested() -> bool:
    """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –±—É–ª–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞ –∑—É–ø–∏–Ω–∫–∞"""
    try:
        stop_flag = redis_client.get("scraping:stop_requested")
        return stop_flag and stop_flag.decode() == "1"
    except Exception:
        return False


@celery_app.task(bind=True, base=CallbackTask, name='scrape_domain_task')
def scrape_domain_task(self, domain: str, session_id: int, config: Optional[Dict] = None) -> Dict:
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω—É
    
    Args:
        domain: –î–æ–º–µ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É
        session_id: ID —Å–µ—Å—ñ—ó –ø–∞—Ä—Å–∏–Ω–≥—É
        config: –î–æ–¥–∞—Ç–∫–æ–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è (proxy, gemini key —Ç–æ—â–æ)
    
    Returns:
        Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥—É
    """
    task_id = self.request.id
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –Ω–µ –±—É–ª–æ –∑–∞–ø—Ä–æ—à–µ–Ω–æ –∑—É–ø–∏–Ω–∫—É
    if _is_stop_requested():
        logger.info(f"[Task {task_id}] ‚èπ –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ {domain} - –∑—É–ø–∏–Ω–∫–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞")
        
        # Update task status with "skipped" terminal state so counters remain consistent
        skipped_result = {
            "success": False,
            "domain": domain,
            "session_id": session_id,
            "deals_count": 0,
            "deals": [],
            "error": "–ó—É–ø–∏–Ω–∫–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞",
            "skipped": True
        }
        _update_task_status(task_id, domain, "skipped", session_id, skipped_result)
        
        # Also update database session counters to keep DB in sync with Redis
        # Without this, processed_domains in DB would be lower than Redis counter
        _update_session_in_db(session_id, skipped_result)
        
        return skipped_result
    
    logger.info(f"[Task {task_id}] –ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É –¥–æ–º–µ–Ω—É: {domain}")
    _add_ui_log("INFO", f"–ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É –¥–æ–º–µ–Ω—É: {domain}", domain)
    
    # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å –≤ Redis
    _update_task_status(task_id, domain, "running", session_id)
    
    try:
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É –æ–±—Ä–æ–±–∫—É
        result = asyncio.run(_scrape_domain_async(domain, session_id, config or {}))
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å
        _update_task_status(task_id, domain, "completed", session_id, result)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å–µ—Å—ñ—é –≤ –ë–î
        _update_session_in_db(session_id, result)
        
        deals_count = result.get('deals_count', 0)
        if result.get('success'):
            logger.info(f"[Task {task_id}] ‚úì –ó–∞–≤–µ—Ä—à–µ–Ω–æ –ø–∞—Ä—Å–∏–Ω–≥ {domain}: {deals_count} —É–≥–æ–¥")
            _add_ui_log("INFO", f"‚úì –ó–∞–≤–µ—Ä—à–µ–Ω–æ –ø–∞—Ä—Å–∏–Ω–≥ {domain}: {deals_count} —É–≥–æ–¥", domain, {"deals_count": deals_count})
        else:
            error_msg = result.get('error', 'Unknown error')
            logger.warning(f"[Task {task_id}] ‚ö† –ü–∞—Ä—Å–∏–Ω–≥ {domain} –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑ –ø–æ–º–∏–ª–∫–æ—é: {error_msg}")
            _add_ui_log("WARNING", f"‚ö† –ü–∞—Ä—Å–∏–Ω–≥ {domain}: {error_msg[:100]}", domain)
        
        return result
    
    except Exception as e:
        logger.error(f"[Task {task_id}] ‚úó –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {domain}: {str(e)}", exc_info=True)
        _add_ui_log("ERROR", f"‚úó –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {domain}: {str(e)[:100]}", domain)
        
        error_result = {
            "success": False,
            "domain": domain,
            "error": str(e),
            "deals_count": 0
        }
        
        _update_task_status(task_id, domain, "failed", session_id, error_result)
        
        # –û–Ω–æ–≤–ª—é—î–º–æ —Å–µ—Å—ñ—é –≤ –ë–î
        _update_session_in_db(session_id, error_result)
        
        return error_result


async def _scrape_domain_async(domain: str, session_id: int, config: Dict) -> Dict:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É –¥–æ–º–µ–Ω—É
    
    –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª: WebScraper ‚Üí GeminiService ‚Üí –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    """
    result = {
        "success": False,
        "domain": domain,
        "session_id": session_id,
        "deals_count": 0,
        "deals": [],
        "error": None,
        "scraped_at": datetime.utcnow().isoformat(),
        "metadata": {}
    }
    
    # –ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ HTML —á–µ—Ä–µ–∑ WebScraper
    scraper = None
    scraped_data = None
    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ scraper –∑ –ø—Ä–æ–∫—Å—ñ —è–∫—â–æ —î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
        proxy_config = config.get('proxy')
        scraper = WebScraper.create_with_config(proxy_config) if proxy_config else WebScraper()
        
        logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è HTML –¥–ª—è {domain}...")
        _add_ui_log("DEBUG", f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è HTML –¥–ª—è {domain}...", domain)
        
        # use_cache=False ‚Äî async Redis –∫–µ—à –¥–∞—î "Event loop is closed" —É Celery
        scraped_data = await scraper.scrape_domain(domain, use_proxy=bool(proxy_config), use_cache=False)
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ WebScraper –¥–ª—è {domain}: {e}")
        _add_ui_log("ERROR", f"WebScraper –ø–æ–º–∏–ª–∫–∞ –¥–ª—è {domain}: {str(e)[:100]}", domain)
        result['error'] = f"WebScraper error: {str(e)}"
    finally:
        # –ó–∞–∫—Ä–∏–≤–∞—î–º–æ HTTP —Å–µ—Å—ñ—é
        if scraper:
            try:
                await scraper.close()
            except Exception:
                pass
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç scraping
    if scraped_data is None:
        return result
    
    if not scraped_data['success']:
        error_msg = scraped_data.get('error', 'Scraping failed')
        result['error'] = error_msg
        _add_ui_log("ERROR", f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {domain}: {error_msg[:100]}", domain)
        return result
    
    html_len = len(scraped_data.get('html_raw', ''))
    result['metadata']['html_length'] = html_len
    _add_ui_log("INFO", f"‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ HTML –¥–ª—è {domain} ({html_len} –±–∞–π—Ç)", domain, {"html_length": html_len})
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑—É–ø–∏–Ω–∫–∏ –ø–µ—Ä–µ–¥ Gemini
    if _is_stop_requested():
        result['error'] = "–ó—É–ø–∏–Ω–∫–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞"
        result['skipped'] = True
        return result
    
    # –ö—Ä–æ–∫ 2: –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —á–µ—Ä–µ–∑ Gemini AI
    try:
        gemini_key = config.get('gemini_key')
        prompt_template = config.get('prompt')
        gemini = GeminiService(
            api_key=gemini_key or None,
            prompt_template=prompt_template
        )
        
        logger.info(f"–ê–Ω–∞–ª—ñ–∑ —á–µ—Ä–µ–∑ Gemini AI –¥–ª—è {domain}...")
        _add_ui_log("DEBUG", f"–ê–Ω–∞–ª—ñ–∑ —á–µ—Ä–µ–∑ Gemini AI –¥–ª—è {domain}...", domain)
        
        deals, error, metadata = await gemini.extract_deals_from_scraped_data(scraped_data)
        
        if error:
            result['error'] = error
            result['metadata']['gemini'] = metadata
            _add_ui_log("WARNING", f"Gemini –ø–æ–º–∏–ª–∫–∞ –¥–ª—è {domain}: {error[:100]}", domain)
            return result
        
        result['success'] = True
        result['deals_count'] = len(deals)
        result['deals'] = [deal.dict() for deal in deals]
        result['metadata']['gemini'] = metadata
        
        logger.info(f"‚úì –ó–Ω–∞–π–¥–µ–Ω–æ {len(deals)} —É–≥–æ–¥ –¥–ª—è {domain}")
        _add_ui_log("INFO", f"‚úì Gemini –∑–Ω–∞–π—à–æ–≤ {len(deals)} —É–≥–æ–¥ –¥–ª—è {domain}", domain, {"deals_count": len(deals)})
        
    except Exception as e:
        err_s = str(e).strip()
        if '"shop"' in err_s or err_s in ('\n    "shop"', '"shop"', "'shop'"):
            msg = "Gemini: –≤—ñ–¥–ø–æ–≤—ñ–¥—å –ø–æ—Ä–æ–∂–Ω—è –∞–±–æ –∑–∞–±–ª–æ–∫–æ–≤–∞–Ω–∞ (–Ω–µ–º–∞—î —Ç–µ–∫—Å—Ç—É –≤ parts)"
        else:
            msg = f"Gemini error: {err_s[:200]}"
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ Gemini –¥–ª—è {domain}: {msg}")
        _add_ui_log("ERROR", f"Gemini –ø–æ–º–∏–ª–∫–∞ –¥–ª—è {domain}: {msg[:100]}", domain)
        result['error'] = msg
        return result
    
    # –ö—Ä–æ–∫ 3: –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ë–î —Ç–∞ Redis
    try:
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ Redis –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É
        _save_scraping_result(session_id, domain, result)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ –ë–î –¥–ª—è –ø–æ—Å—Ç—ñ–π–Ω–æ–≥–æ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è
        if result['success'] and result['deals_count'] > 0:
            from app.db.session import SessionLocal
            from app.db import crud
            
            db = SessionLocal()
            try:
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–∂–Ω—É —É–≥–æ–¥—É –≤ –ë–î
                for deal_data in result['deals']:
                    crud.create_scraped_deal(
                        db=db,
                        session_id=session_id,
                        domain=domain,
                        deal_data=deal_data
                    )
                logger.info(f"‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ {result['deals_count']} —É–≥–æ–¥ –≤ –ë–î –¥–ª—è {domain}")
            except Exception as db_error:
                logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –ë–î: {db_error}")
            finally:
                db.close()
    except Exception as e:
        logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {e}")
    
    # –ö—Ä–æ–∫ 4: –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ webhook
    if result['success'] and result['deals_count'] > 0:
        try:
            from app.services.webhook import WebhookService
            
            webhook_config = config.get('webhook', {})
            webhook = WebhookService.create_from_config(webhook_config)
            
            logger.info(f"–í—ñ–¥–ø—Ä–∞–≤–∫–∞ {result['deals_count']} —É–≥–æ–¥ –≤ webhook...")
            _add_ui_log("DEBUG", f"–í—ñ–¥–ø—Ä–∞–≤–∫–∞ {result['deals_count']} —É–≥–æ–¥ –≤ webhook –¥–ª—è {domain}...", domain)
            
            webhook_result = await webhook.send_deals_from_scraping_result(result, session_id)
            
            result['webhook_sent'] = webhook_result['successful'] > 0
            result['webhook_stats'] = webhook_result
            
            successful = webhook_result['successful']
            total = webhook_result['total']
            logger.info(f"Webhook: {successful}/{total} —É—Å–ø—ñ—à–Ω–∏—Ö")
            
            if successful > 0:
                _add_ui_log("INFO", f"‚úì Webhook: –≤—ñ–¥–ø—Ä–∞–≤–ª–µ–Ω–æ {successful}/{total} —É–≥–æ–¥ –¥–ª—è {domain}", domain, {"successful": successful, "total": total})
            else:
                _add_ui_log("WARNING", f"‚ö† Webhook: 0/{total} —É–≥–æ–¥ –¥–ª—è {domain}", domain)
                
        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –≤ webhook: {e}")
            _add_ui_log("ERROR", f"Webhook –ø–æ–º–∏–ª–∫–∞ –¥–ª—è {domain}: {str(e)[:100]}", domain)
            result['webhook_sent'] = False
            result['webhook_error'] = str(e)
    
    return result


def _update_task_status(task_id: str, domain: str, status: str, session_id: int, result: Optional[Dict] = None):
    """–û–Ω–æ–≤–∏—Ç–∏ —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á—ñ –≤ Redis"""
    try:
        key = f"task:{task_id}"
        data = {
            "task_id": task_id,
            "domain": domain,
            "status": status,
            "session_id": session_id,
            "updated_at": datetime.utcnow().isoformat()
        }
        
        if result:
            data['result'] = result
        
        redis_client.setex(key, 3600, json.dumps(data))  # TTL 1 –≥–æ–¥–∏–Ω–∞
        
        # –¢–∞–∫–æ–∂ –æ–Ω–æ–≤–ª—é—î–º–æ –∑–∞–≥–∞–ª—å–Ω–∏–π –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó
        _update_session_progress(session_id, status, domain)
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É –∑–∞–¥–∞—á—ñ: {e}")


def _update_session_progress(session_id: int, status: str, domain: str):
    """
    –û–Ω–æ–≤–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó –ø–∞—Ä—Å–∏–Ω–≥—É (–∞—Ç–æ–º–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó —á–µ—Ä–µ–∑ Redis Lua script)
    
    Uses a Lua script for atomic get-and-set to prevent race conditions.
    Also handles "skipped" status as a terminal state (counts as processed).
    """
    try:
        counters_key = f"session:{session_id}:counters"
        domains_key = f"session:{session_id}:domain_status"
        
        # Lua script for atomic status update and counter adjustment
        # This prevents TOCTOU race conditions between hget and hset
        lua_script = """
        local domains_key = KEYS[1]
        local counters_key = KEYS[2]
        local domain = ARGV[1]
        local new_status = ARGV[2]
        local updated_at = ARGV[3]
        
        -- Atomically get old status and set new status
        local old_status = redis.call('HGET', domains_key, domain)
        redis.call('HSET', domains_key, domain, new_status)
        redis.call('EXPIRE', domains_key, 7200)
        
        -- Adjust running counter if old status was "running"
        if old_status == "running" then
            redis.call('HINCRBY', counters_key, 'running', -1)
        end
        
        -- Adjust counters based on new status
        if new_status == "running" then
            redis.call('HINCRBY', counters_key, 'running', 1)
        elseif new_status == "completed" then
            redis.call('HINCRBY', counters_key, 'processed', 1)
            redis.call('HINCRBY', counters_key, 'successful', 1)
        elseif new_status == "failed" then
            redis.call('HINCRBY', counters_key, 'processed', 1)
            redis.call('HINCRBY', counters_key, 'failed', 1)
        elseif new_status == "skipped" then
            -- Skipped tasks are terminal states and count as processed
            redis.call('HINCRBY', counters_key, 'processed', 1)
            redis.call('HINCRBY', counters_key, 'skipped', 1)
        end
        
        -- Update timestamp
        redis.call('HSET', counters_key, 'updated_at', updated_at)
        redis.call('EXPIRE', counters_key, 7200)
        
        return old_status
        """
        
        # Execute the Lua script atomically
        redis_client.eval(
            lua_script,
            2,  # Number of keys
            domains_key,
            counters_key,
            domain,
            status,
            datetime.utcnow().isoformat()
        )
        
    except Exception as e:
        logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É —Å–µ—Å—ñ—ó: {e}")


def _save_scraping_result(session_id: int, domain: str, result: Dict):
    """–ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥—É –≤ Redis"""
    try:
        key = f"session:{session_id}:results:{domain}"
        redis_client.setex(key, 7200, json.dumps(result))  # TTL 2 –≥–æ–¥–∏–Ω–∏
        
        # –î–æ–¥–∞—î–º–æ –¥–æ–º–µ–Ω –¥–æ —Å–ø–∏—Å–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Å–µ—Å—ñ—ó
        list_key = f"session:{session_id}:domains"
        redis_client.sadd(list_key, domain)
        redis_client.expire(list_key, 7200)
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É: {e}")


def _update_session_in_db(session_id: int, result: Dict):
    """
    –û–Ω–æ–≤–∏—Ç–∏ —Å–µ—Å—ñ—é –ø–∞—Ä—Å–∏–Ω–≥—É –≤ –ë–î (–∞—Ç–æ–º–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó —á–µ—Ä–µ–∑ SQL UPDATE)
    """
    try:
        from app.db.session import SessionLocal
        from app.db import crud
        
        db = SessionLocal()
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞—Ç–æ–º–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ª—ñ—á–∏–ª—å–Ω–∏–∫—ñ–≤
            session = crud.atomic_increment_session_counters(
                db=db,
                session_id=session_id,
                success=result.get('success', False)
            )
            
            if session:
                # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç—É—Å –≤ Redis
                if session.status == "completed":
                    redis_client.set("scraping:status", "completed")
                    # –û—á–∏—â–∞—î–º–æ active_session —â–æ–± –¥–æ–∑–≤–æ–ª–∏—Ç–∏ –Ω–∞—Å—Ç—É–ø–Ω–∏–π –∑–∞–ø—É—Å–∫
                    redis_client.delete("parsing:active_session")
                    logger.info(f"‚úì –°–µ—Å—ñ—è {session_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞, active_session –æ—á–∏—â–µ–Ω–æ")
                elif session.status == "failed":
                    redis_client.set("scraping:status", "failed")
                    # –¢–∞–∫–æ–∂ –æ—á–∏—â–∞—î–º–æ –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
                    redis_client.delete("parsing:active_session")
                
                logger.debug(
                    f"–û–Ω–æ–≤–ª–µ–Ω–æ —Å–µ—Å—ñ—é {session_id}: "
                    f"processed={session.processed_domains}, "
                    f"successful={session.successful_domains}, "
                    f"failed={session.failed_domains}"
                )
        finally:
            db.close()
    except Exception as e:
        logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–µ—Å—ñ—ó –≤ –ë–î: {e}")


@celery_app.task(name='start_batch_scraping')
def start_batch_scraping(domains: List[str], session_id: int, config: Optional[Dict] = None) -> Dict:
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–∞–∫–µ—Ç–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–º–µ–Ω—ñ–≤
    
    Args:
        domains: –°–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω—ñ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É
        session_id: ID —Å–µ—Å—ñ—ó –ø–∞—Ä—Å–∏–Ω–≥—É
        config: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è –≤—Å—ñ—Ö –∑–∞–¥–∞—á
    
    Returns:
        Dict –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –∑–∞–ø—É—â–µ–Ω—ñ –∑–∞–¥–∞—á—ñ
    """
    logger.info(f"–ó–∞–ø—É—Å–∫ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥—É: {len(domains)} –¥–æ–º–µ–Ω—ñ–≤, —Å–µ—Å—ñ—è {session_id}")
    
    # –û—á–∏—â–∞—î–º–æ —Ñ–ª–∞–≥ –∑—É–ø–∏–Ω–∫–∏ –≤—ñ–¥ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö —Å–µ—Å—ñ–π
    redis_client.delete("scraping:stop_requested")
    
    # –õ–æ–≥—É—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é (–±–µ–∑ –ø–∞—Ä–æ–ª—ñ–≤)
    proxy_info = "–ë–µ–∑ –ø—Ä–æ–∫—Å—ñ"
    if config and config.get('proxy') and config['proxy'].get('host'):
        proxy_info = f"–ü—Ä–æ–∫—Å—ñ: {config['proxy']['host']}:{config['proxy'].get('http_port', 59100)}"
    
    _add_ui_log("INFO", f"üöÄ –°—Ç–∞—Ä—Ç –ø–∞—Ä—Å–∏–Ω–≥—É: {len(domains)} –¥–æ–º–µ–Ω—ñ–≤, —Å–µ—Å—ñ—è #{session_id}", extra={
        "session_id": session_id,
        "total_domains": len(domains),
        "proxy": proxy_info
    })
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó
    _init_session_progress(session_id, domains)
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ –∑–∞–¥–∞—á—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–æ–º–µ–Ω—É
    task_ids = []
    task_id_list = []  # –î–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ Redis
    for domain in domains:
        task = scrape_domain_task.delay(domain, session_id, config)
        task_ids.append({
            "task_id": task.id,
            "domain": domain
        })
        task_id_list.append(task.id)
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ task_ids –≤ Redis –¥–ª—è –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —Å–∫–∞—Å—É–≤–∞–Ω–Ω—è
    redis_client.set("scraping:task_ids", json.dumps(task_id_list))
    
    logger.info(f"–ó–∞–ø—É—â–µ–Ω–æ {len(task_ids)} –∑–∞–¥–∞—á –¥–ª—è —Å–µ—Å—ñ—ó {session_id}")
    _add_ui_log("INFO", f"üìã –ó–∞–ø—É—â–µ–Ω–æ {len(task_ids)} –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–æ–±–∫–∏", extra={"task_count": len(task_ids)})
    
    return {
        "session_id": session_id,
        "total_domains": len(domains),
        "task_ids": task_ids,
        "started_at": datetime.utcnow().isoformat()
    }


def _init_session_progress(session_id: int, domains: List[str]):
    """
    –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Redis hashes –¥–ª—è –∞—Ç–æ–º–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π)
    """
    try:
        counters_key = f"session:{session_id}:counters"
        domains_key = f"session:{session_id}:domain_status"
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫–∏ (including skipped)
        counters = {
            "total": len(domains),
            "processed": 0,
            "successful": 0,
            "failed": 0,
            "skipped": 0,
            "running": 0,
            "started_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat()
        }
        redis_client.hset(counters_key, mapping=counters)
        redis_client.expire(counters_key, 7200)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ —Å—Ç–∞—Ç—É—Å–∏ –¥–æ–º–µ–Ω—ñ–≤ (–±–∞—Ç—á–∞–º–∏ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö —Å–ø–∏—Å–∫—ñ–≤)
        if domains:
            domain_statuses = {domain: "pending" for domain in domains}
            redis_client.hset(domains_key, mapping=domain_statuses)
            redis_client.expire(domains_key, 7200)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å—Ç–∞—Ç—É—Å —Å–µ—Å—ñ—ó
        redis_client.set("scraping:status", "running")
        redis_client.set("scraping:session_id", session_id)
        
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ø—Ä–æ–≥—Ä–µ—Å—É: {e}")


@celery_app.task(name='get_session_progress')
def get_session_progress(session_id: int) -> Optional[Dict]:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å —Å–µ—Å—ñ—ó –ø–∞—Ä—Å–∏–Ω–≥—É
    
    Args:
        session_id: ID —Å–µ—Å—ñ—ó
    
    Returns:
        Dict –∑ –ø—Ä–æ–≥—Ä–µ—Å–æ–º –∞–±–æ None
    """
    try:
        counters_key = f"session:{session_id}:counters"
        domains_key = f"session:{session_id}:domain_status"
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ª—ñ—á–∏–ª—å–Ω–∏–∫–∏
        counters = redis_client.hgetall(counters_key)
        if not counters:
            return None
        
        # –î–µ–∫–æ–¥—É—î–º–æ bytes -> str
        def decode_val(v):
            return v.decode('utf-8') if isinstance(v, bytes) else v
        
        counters = {decode_val(k): decode_val(v) for k, v in counters.items()}
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Å—Ç–∞—Ç—É—Å–∏ –¥–æ–º–µ–Ω—ñ–≤
        domains = redis_client.hgetall(domains_key)
        domains = {decode_val(k): decode_val(v) for k, v in domains.items()}
        
        return {
            "session_id": session_id,
            "total": int(counters.get("total", 0)),
            "processed": int(counters.get("processed", 0)),
            "successful": int(counters.get("successful", 0)),
            "failed": int(counters.get("failed", 0)),
            "skipped": int(counters.get("skipped", 0)),  # Include skipped counter
            "running": int(counters.get("running", 0)),
            "updated_at": counters.get("updated_at"),
            "domains": domains
        }
    except Exception as e:
        logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É: {e}")
        return None


@celery_app.task(name='cleanup_old_sessions')
def cleanup_old_sessions():
    """
    –û—á–∏—Å—Ç–∏—Ç–∏ —Å—Ç–∞—Ä—ñ –¥–∞–Ω—ñ —Å–µ—Å—ñ–π –∑ Redis
    
    –ü–µ—Ä—ñ–æ–¥–∏—á–Ω–∞ –∑–∞–¥–∞—á–∞ –¥–ª—è Celery Beat
    """
    logger.info("–û—á–∏—â–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö —Å–µ—Å—ñ–π...")
    # TODO: –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ª–æ–≥—ñ–∫—É –æ—á–∏—â–µ–Ω–Ω—è
    pass
