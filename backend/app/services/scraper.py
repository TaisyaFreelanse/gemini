import aiohttp
import asyncio
from bs4 import BeautifulSoup
from typing import Optional, Dict, Tuple
import logging
from urllib.parse import urlparse, urljoin
import re
from app.services.proxy import ProxyRotator, ProxyConfig
from app.core.config import settings
from app.core.cache import get_cache

logger = logging.getLogger(__name__)


class WebScraper:
    """
    –í–µ–±-—Å–∫—Ä–∞–ø–µ—Ä –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –ø—Ä–æ–∫—Å—ñ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Å–∞–π—Ç—ñ–≤
    
    –û—Å–Ω–æ–≤–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ:
    - –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ HTTP/HTTPS —Ç–∞ SOCKS5 –ø—Ä–æ–∫—Å—ñ
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ —Ä–æ—Ç–∞—Ü—ñ—è –ø—Ä–æ–∫—Å—ñ –ø—Ä–∏ –ø–æ–º–∏–ª–∫–∞—Ö
    - –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ –≤–∏–¥–∏–º–æ–≥–æ HTML –∫–æ–Ω—Ç–µ–Ω—Ç—É
    - –û–±—Ä–æ–±–∫–∞ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à–æ–≥–æ —Ä—ñ–≤–Ω—è (–≥–æ–ª–æ–≤–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞)
    - Timeout: 30 —Å–µ–∫—É–Ω–¥
    - Retry logic: –º–∞–∫—Å–∏–º—É–º 3 —Å–ø—Ä–æ–±–∏
    """
    
    # –°–ø–∏—Å–æ–∫ —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∏—Ö User-Agent –¥–ª—è —Ä–æ—Ç–∞—Ü—ñ—ó
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    ]
    
    def __init__(self, proxy_rotator: Optional[ProxyRotator] = None):
        import random
        self.proxy_rotator = proxy_rotator
        # connect=15 ‚Äî —á–∞—Å –Ω–∞ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ –ø—Ä–æ–∫—Å—ñ; total ‚Äî –Ω–∞ –≤–µ—Å—å –∑–∞–ø–∏—Ç
        self.timeout = aiohttp.ClientTimeout(
            total=settings.SCRAPING_TIMEOUT,
            connect=15,
            sock_connect=15,
        )
        self.max_retries = settings.SCRAPING_MAX_RETRIES
        self._random = random
    
    def _get_headers(self, url: str) -> dict:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ headers –∑ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º User-Agent"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc or parsed.path.split('/')[0]
        
        return {
            'User-Agent': self._random.choice(self.USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Referer': f'https://www.google.com/search?q={domain}',
        }
    
    async def _try_playwright(self, url: str) -> Tuple[Optional[str], Optional[str]]:
        """
        –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É —á–µ—Ä–µ–∑ Playwright (headless browser)
        –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —è–∫ fallback –ø—Ä–∏ 403 –ø–æ–º–∏–ª–∫–∞—Ö
        """
        try:
            from app.services.playwright_scraper import fetch_with_playwright
            
            # –û—Ç—Ä–∏–º—É—î–º–æ proxy config —è–∫—â–æ —î
            proxy_config = None
            if self.proxy_rotator and self.proxy_rotator.proxy_configs:
                proxy = self.proxy_rotator.proxy_configs[0]
                proxy_config = {
                    'host': proxy.host,
                    'http_port': proxy.http_port,
                    'login': proxy.login,
                    'password': proxy.password
                }
            
            return await fetch_with_playwright(url, proxy_config=proxy_config, timeout=15000)
        except Exception as e:
            logger.error(f"Playwright fallback –ø–æ–º–∏–ª–∫–∞: {e}")
            return None, f"Playwright error: {str(e)[:100]}"
    
    async def fetch_website(self, url: str, use_proxy: bool = True) -> Tuple[Optional[str], Optional[str]]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∑ –≤–∫–∞–∑–∞–Ω–æ–≥–æ URL
        
        Args:
            url: URL —Å–∞–π—Ç—É –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É
            use_proxy: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø—Ä–æ–∫—Å—ñ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º True)
        
        Returns:
            Tuple[html_content, error_message]
            - html_content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∞–±–æ None –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
            - error_message: –ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –ø–æ–º–∏–ª–∫—É –∞–±–æ None –ø—Ä–∏ —É—Å–ø—ñ—Ö—É
        """
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ URL
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        for attempt in range(self.max_retries):
            proxy_base_url = None
            proxy_auth = None

            try:
                # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ–∫—Å—ñ (base_url + auth –æ–∫—Ä–µ–º–æ ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è aiohttp)
                if use_proxy and self.proxy_rotator:
                    parts = self.proxy_rotator.get_next_proxy_for_aiohttp(proxy_type="http")
                    if not parts:
                        logger.error("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø—Ä–æ–∫—Å—ñ")
                        return None, "–í—Å—ñ –ø—Ä–æ–∫—Å—ñ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ"
                    proxy_base_url, login, password = parts
                    proxy_auth = aiohttp.BasicAuth(login, password) if (login and password) else None

                # –í–∏–∫–æ–Ω—É—î–º–æ HTTP –∑–∞–ø–∏—Ç
                headers = self._get_headers(url)
                
                # –í–∏–º–∏–∫–∞—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É SSL –ø—Ä–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—ñ –ø—Ä–æ–∫—Å—ñ (—á–∞—Å—Ç–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –¥–ª—è datacenter –ø—Ä–æ–∫—Å—ñ)
                import ssl
                ssl_context = ssl.create_default_context()
                if proxy_base_url:
                    ssl_context.check_hostname = False
                    ssl_context.verify_mode = ssl.CERT_NONE
                
                connector = aiohttp.TCPConnector(ssl=ssl_context)
                async with aiohttp.ClientSession(timeout=self.timeout, connector=connector) as session:
                    kwargs = {'headers': headers}
                    if proxy_base_url:
                        kwargs['proxy'] = proxy_base_url
                        if proxy_auth:
                            kwargs['proxy_auth'] = proxy_auth

                    logger.info(
                        f"–°–ø—Ä–æ–±–∞ {attempt + 1}/{self.max_retries}: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {url}" +
                        (f" —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å—ñ {proxy_base_url}" if proxy_base_url else "")
                    )

                    async with session.get(url, **kwargs) as response:
                        if response.status == 200:
                            html_content = await response.text()
                            
                            # –£—Å–ø—ñ—à–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è - –≤—ñ–¥–º—ñ—á–∞—î–º–æ –ø—Ä–æ–∫—Å—ñ —è–∫ —Ä–æ–±–æ—á–∏–π
                            if proxy_base_url and self.proxy_rotator:
                                self.proxy_rotator.mark_proxy_success(proxy_base_url)
                            
                            logger.info(f"‚úì –£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {url} ({len(html_content)} –±–∞–π—Ç)")
                            return html_content, None
                        
                        else:
                            error_msg = f"HTTP {response.status}: {response.reason}"
                            logger.warning(f"‚úó {error_msg} –¥–ª—è {url}")
                            
                            # 403 - –∞–Ω—Ç–∏–±–æ—Ç –∑–∞—Ö–∏—Å—Ç, –ø—Ä–æ–±—É—î–º–æ Playwright
                            if response.status == 403:
                                logger.info(f"üåê –ü—Ä–æ–±—É—î–º–æ Playwright –¥–ª—è {url} (–∞–Ω—Ç–∏–±–æ—Ç 403)")
                                playwright_html, playwright_error = await self._try_playwright(url)
                                if playwright_html:
                                    return playwright_html, None
                                else:
                                    logger.warning(f"Playwright —Ç–µ–∂ –Ω–µ –∑–º—ñ–≥: {playwright_error}")
                                    return None, f"403 + Playwright failed: {playwright_error}"
                            
                            # 429 - rate limit, –ø–æ–≤—Ç–æ—Ä—é—î–º–æ
                            if response.status == 429:
                                if proxy_base_url and self.proxy_rotator:
                                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)
                                # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ retry loop
                            # –Ü–Ω—à—ñ 4xx –ø–æ–º–∏–ª–∫–∏ - –Ω–µ –ø–æ–≤—Ç–æ—Ä—é—î–º–æ
                            elif 400 <= response.status < 500:
                                return None, error_msg
                            else:
                                # 5xx –ø–æ–º–∏–ª–∫–∏ - –ø–æ–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–æ–∫—Å—ñ —è–∫ –Ω–µ–≤–¥–∞–ª–∏–π
                                if proxy_base_url and self.proxy_rotator:
                                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)

            except asyncio.TimeoutError:
                error_msg = f"Timeout –ø—ñ—Å–ª—è {settings.SCRAPING_TIMEOUT} —Å–µ–∫—É–Ω–¥"
                logger.warning(f"‚úó {error_msg} –¥–ª—è {url}")
                if proxy_base_url and self.proxy_rotator:
                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)

            except aiohttp.ClientError as e:
                error_msg = f"–ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è ({type(e).__name__}): {str(e)}"
                logger.warning(f"‚úó {error_msg} –¥–ª—è {url}" + (f" (–ø—Ä–æ–∫—Å—ñ {proxy_base_url})" if proxy_base_url else ""))
                if proxy_base_url and self.proxy_rotator:
                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)

            except Exception as e:
                error_msg = f"–ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {str(e)}"
                logger.error(f"‚úó {error_msg} –¥–ª—è {url}", exc_info=True)
                if proxy_base_url and self.proxy_rotator:
                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)
            
            # –ß–µ–∫–∞—î–º–æ –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–æ—é —Å–ø—Ä–æ–±–æ—é (exponential backoff)
            if attempt < self.max_retries - 1:
                wait_time = 2 ** attempt  # 1s, 2s, 4s
                logger.info(f"–ß–µ–∫–∞—î–º–æ {wait_time}—Å –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–æ—é —Å–ø—Ä–æ–±–æ—é...")
                await asyncio.sleep(wait_time)
        
        return None, f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ø—ñ—Å–ª—è {self.max_retries} —Å–ø—Ä–æ–±"
    
    def extract_visible_content(self, html: str, base_url: str) -> Dict[str, any]:
        """
        –í–∏—Ç—è–≥–Ω—É—Ç–∏ –≤–∏–¥–∏–º–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∑ HTML
        
        Args:
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
            base_url: –ë–∞–∑–æ–≤–∏–π URL –¥–ª—è —Ä–µ–∑–æ–ª—é—Ü—ñ—ó –≤—ñ–¥–Ω–æ—Å–Ω–∏—Ö –ø–æ—Å–∏–ª–∞–Ω—å
        
        Returns:
            Dict –∑ –≤–∏—Ç—è–≥–Ω—É—Ç–∏–º–∏ –¥–∞–Ω–∏–º–∏:
            - title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            - text: –í–∏–¥–∏–º–∏–π —Ç–µ–∫—Å—Ç
            - links: –°–ø–∏—Å–æ–∫ –ø–æ—Å–∏–ª–∞–Ω—å
            - meta_description: Meta –æ–ø–∏—Å
            - clean_html: –û—á–∏—â–µ–Ω–∏–π HTML (–±–µ–∑ scripts, styles)
        """
        soup = BeautifulSoup(html, 'lxml')
        
        # –í–∏–¥–∞–ª—è—î–º–æ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ç–µ–≥–∏
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe', 'noscript']):
            tag.decompose()
        
        # –í–∏—Ç—è–≥—É—î–º–æ title
        title = soup.title.string if soup.title else ""
        
        # –í–∏—Ç—è–≥—É—î–º–æ meta description
        meta_desc = ""
        meta_tag = soup.find('meta', attrs={'name': 'description'})
        if meta_tag and meta_tag.get('content'):
            meta_desc = meta_tag.get('content')
        
        # –í–∏—Ç—è–≥—É—î–º–æ –≤–∏–¥–∏–º–∏–π —Ç–µ–∫—Å—Ç
        text = soup.get_text(separator=' ', strip=True)
        # –û—á–∏—â–∞—î–º–æ –º–Ω–æ–∂–∏–Ω–Ω—ñ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –í–∏—Ç—è–≥—É—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
        links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            # –†–µ–∑–æ–ª–≤–∏–º–æ –≤—ñ–¥–Ω–æ—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
            absolute_url = urljoin(base_url, href)
            link_text = link.get_text(strip=True)
            if link_text:
                links.append({
                    'url': absolute_url,
                    'text': link_text
                })
        
        # –û—á–∏—â–µ–Ω–∏–π HTML (–¥–ª—è Gemini)
        clean_html = str(soup)
        
        return {
            'title': title.strip() if title else "",
            'text': text[:50000],  # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä —Ç–µ–∫—Å—Ç—É
            'links': links[:100],  # –û–±–º–µ–∂—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Å–∏–ª–∞–Ω—å
            'meta_description': meta_desc.strip() if meta_desc else "",
            'clean_html': clean_html[:100000]  # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä HTML –¥–ª—è Gemini
        }
    
    async def scrape_domain(self, domain: str, use_proxy: bool = True, use_cache: bool = True) -> Dict[str, any]:
        """
        –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥—É –¥–æ–º–µ–Ω—É –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –∫–µ—à—É–≤–∞–Ω–Ω—è
        
        Args:
            domain: –î–æ–º–µ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É (–º–æ–∂–Ω–∞ –∑ –∞–±–æ –±–µ–∑ https://)
            use_proxy: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø—Ä–æ–∫—Å—ñ
            use_cache: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Redis –∫–µ—à (TTL: 1 –≥–æ–¥–∏–Ω–∞)
        
        Returns:
            Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            - success: bool - —á–∏ —É—Å–ø—ñ—à–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥
            - domain: str - –¥–æ–º–µ–Ω
            - url: str - –ø–æ–≤–Ω–∏–π URL
            - html_raw: str - —Å–∏—Ä–∏–π HTML (–º–æ–∂–µ –±—É—Ç–∏ None)
            - content: dict - –≤–∏—Ç—è–≥–Ω—É—Ç–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (–º–æ–∂–µ –±—É—Ç–∏ None)
            - error: str - –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –ø–æ–º–∏–ª–∫—É (–º–æ–∂–µ –±—É—Ç–∏ None)
            - cached: bool - —á–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ –∑ –∫–µ—à—É
        """
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–æ–º–µ–Ω
        if not domain.startswith(('http://', 'https://')):
            url = 'https://' + domain
        else:
            url = domain
            domain = urlparse(url).netloc
        
        result = {
            'success': False,
            'domain': domain,
            'url': url,
            'html_raw': None,
            'content': None,
            'error': None,
            'cached': False
        }
        
        # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∑ –∫–µ—à—É
        if use_cache:
            try:
                cache = await get_cache()
                cached_data = await cache.get_html(domain)
                
                if cached_data:
                    result['success'] = True
                    result['html_raw'] = cached_data.get('html_raw')
                    result['content'] = cached_data.get('content')
                    result['cached'] = True
                    logger.info(f"‚úì –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –∫–µ—à –¥–ª—è {domain}")
                    return result
            except Exception as e:
                logger.warning(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∫–µ—à—É: {e}")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ HTML
        html, error = await self.fetch_website(url, use_proxy=use_proxy)
        
        if html:
            result['success'] = True
            result['html_raw'] = html
            result['content'] = self.extract_visible_content(html, url)
            
            # –ó–±–µ—Ä–µ–≥—Ç–∏ –≤ –∫–µ—à
            if use_cache:
                try:
                    cache = await get_cache()
                    await cache.set_html(domain, {
                        'html_raw': html,
                        'content': result['content']
                    })
                except Exception as e:
                    logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É –≤ –∫–µ—à: {e}")
        else:
            result['error'] = error
        
        return result
    
    @classmethod
    def create_with_config(cls, proxy_config: Optional[Dict] = None) -> "WebScraper":
        """
        –°—Ç–≤–æ—Ä–∏—Ç–∏ WebScraper –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é –ø—Ä–æ–∫—Å—ñ
        
        Args:
            proxy_config: Dict –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏ –ø—Ä–æ–∫—Å—ñ –∞–±–æ None
        
        Returns:
            WebScraper instance
        """
        proxy_rotator = None
        
        if proxy_config:
            proxy_rotator = ProxyRotator.from_config(proxy_config)
        
        return cls(proxy_rotator=proxy_rotator)
