import aiohttp
import asyncio
import ssl
import random
from bs4 import BeautifulSoup
from typing import Optional, Dict, Tuple, Any
import logging
from urllib.parse import urlparse, urljoin
import re
from app.services.proxy import ProxyRotator, ProxyConfig
from app.core.config import settings
from app.core.cache import get_cache

logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏
MAX_TEXT_LENGTH = 50000
MAX_LINKS_COUNT = 100
MAX_HTML_LENGTH = 100000
BACKOFF_BASE = 2
BACKOFF_MAX = 30
BACKOFF_JITTER = 0.1


class WebScraper:
    """
    –í–µ–±-—Å–∫—Ä–∞–ø–µ—Ä –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –ø—Ä–æ–∫—Å—ñ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Å–∞–π—Ç—ñ–≤
    
    –û—Å–Ω–æ–≤–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ:
    - –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ HTTP/HTTPS —Ç–∞ SOCKS5 –ø—Ä–æ–∫—Å—ñ
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ —Ä–æ—Ç–∞—Ü—ñ—è –ø—Ä–æ–∫—Å—ñ –ø—Ä–∏ –ø–æ–º–∏–ª–∫–∞—Ö
    - –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ –≤–∏–¥–∏–º–æ–≥–æ HTML –∫–æ–Ω—Ç–µ–Ω—Ç—É
    - –û–±—Ä–æ–±–∫–∞ —Ç—ñ–ª—å–∫–∏ –ø–µ—Ä—à–æ–≥–æ —Ä—ñ–≤–Ω—è (–≥–æ–ª–æ–≤–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞)
    - Timeout: 30 —Å–µ–∫—É–Ω–¥
    - Retry logic: –º–∞–∫—Å–∏–º—É–º 3 —Å–ø—Ä–æ–±–∏
    """
    
    # –°–ø–∏—Å–æ–∫ —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∏—Ö User-Agent –¥–ª—è —Ä–æ—Ç–∞—Ü—ñ—ó
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    ]
    
    def __init__(self, proxy_rotator: Optional[ProxyRotator] = None):
        self.proxy_rotator = proxy_rotator
        self.timeout = aiohttp.ClientTimeout(
            total=settings.SCRAPING_TIMEOUT,
            connect=15,
            sock_connect=15,
        )
        self.max_retries = settings.SCRAPING_MAX_RETRIES
        
        # Separate session pools for proxy and non-proxy connections
        # This avoids issues with SSL context being shared between different modes
        self._session_no_proxy: Optional[aiohttp.ClientSession] = None
        self._session_with_proxy: Optional[aiohttp.ClientSession] = None
        self._connector_no_proxy: Optional[aiohttp.TCPConnector] = None
        self._connector_with_proxy: Optional[aiohttp.TCPConnector] = None
        self._ssl_context_no_proxy: Optional[ssl.SSLContext] = None
        self._ssl_context_with_proxy: Optional[ssl.SSLContext] = None
    
    def _create_ssl_context(self, for_proxy: bool = False) -> ssl.SSLContext:
        """
        Create a proper SSL context.
        Never disables SSL verification - uses system CA bundle.
        For proxy scenarios, still uses proper SSL with system certificates.
        """
        ssl_context = ssl.create_default_context()
        # Always use proper SSL verification with system CA bundle
        # ssl_context.check_hostname = True  # Already default
        # ssl_context.verify_mode = ssl.CERT_REQUIRED  # Already default
        
        # Load system certificates
        ssl_context.load_default_certs()
        
        return ssl_context
    
    async def _get_session(self, use_proxy: bool = False) -> aiohttp.ClientSession:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –∞–±–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ HTTP —Å–µ—Å—ñ—é –∑ connection pooling.
        Maintains separate sessions for proxy and non-proxy connections.
        """
        if use_proxy:
            if self._session_with_proxy is None or self._session_with_proxy.closed:
                # Create proper SSL context (never disable verification)
                self._ssl_context_with_proxy = self._create_ssl_context(for_proxy=True)
                
                # Connector –∑ –ø—É–ª–æ–º –∑'—î–¥–Ω–∞–Ω—å for proxy
                self._connector_with_proxy = aiohttp.TCPConnector(
                    ssl=self._ssl_context_with_proxy,
                    limit=100,              # –ú–∞–∫—Å–∏–º—É–º –∑'—î–¥–Ω–∞–Ω—å
                    limit_per_host=10,      # –ù–∞ —Ö–æ—Å—Ç
                    ttl_dns_cache=300,      # DNS –∫–µ—à 5 —Ö–≤
                    enable_cleanup_closed=True
                )
                self._session_with_proxy = aiohttp.ClientSession(
                    timeout=self.timeout,
                    connector=self._connector_with_proxy
                )
            return self._session_with_proxy
        else:
            if self._session_no_proxy is None or self._session_no_proxy.closed:
                # Create proper SSL context
                self._ssl_context_no_proxy = self._create_ssl_context(for_proxy=False)
                
                # Connector –∑ –ø—É–ª–æ–º –∑'—î–¥–Ω–∞–Ω—å for non-proxy
                self._connector_no_proxy = aiohttp.TCPConnector(
                    ssl=self._ssl_context_no_proxy,
                    limit=100,              # –ú–∞–∫—Å–∏–º—É–º –∑'—î–¥–Ω–∞–Ω—å
                    limit_per_host=10,      # –ù–∞ —Ö–æ—Å—Ç
                    ttl_dns_cache=300,      # DNS –∫–µ—à 5 —Ö–≤
                    enable_cleanup_closed=True
                )
                self._session_no_proxy = aiohttp.ClientSession(
                    timeout=self.timeout,
                    connector=self._connector_no_proxy
                )
            return self._session_no_proxy
    
    async def close(self):
        """–ó–∞–∫—Ä–∏—Ç–∏ —Å–µ—Å—ñ—ó —Ç–∞ –∑–≤—ñ–ª—å–Ω–∏—Ç–∏ —Ä–µ—Å—É—Ä—Å–∏"""
        # Close non-proxy session
        if self._session_no_proxy and not self._session_no_proxy.closed:
            await self._session_no_proxy.close()
        self._session_no_proxy = None
        self._connector_no_proxy = None
        
        # Close proxy session
        if self._session_with_proxy and not self._session_with_proxy.closed:
            await self._session_with_proxy.close()
        self._session_with_proxy = None
        self._connector_with_proxy = None
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()
    
    def _get_headers(self, url: str) -> dict:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ headers –∑ –≤–∏–ø–∞–¥–∫–æ–≤–∏–º User-Agent"""
        parsed = urlparse(url)
        domain = parsed.netloc or parsed.path.split('/')[0]
        
        return {
            'User-Agent': random.choice(self.USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Referer': f'https://www.google.com/search?q={domain}',
        }
    
    async def _try_playwright(self, url: str) -> Tuple[Optional[str], Optional[str]]:
        """
        –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É —á–µ—Ä–µ–∑ Playwright (headless browser)
        –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —è–∫ fallback –ø—Ä–∏ 403 –ø–æ–º–∏–ª–∫–∞—Ö
        """
        try:
            from app.services.playwright_scraper import fetch_with_playwright
            
            # –û—Ç—Ä–∏–º—É—î–º–æ proxy config —è–∫—â–æ —î
            proxy_config = None
            if self.proxy_rotator and self.proxy_rotator.proxy_configs:
                proxy = self.proxy_rotator.proxy_configs[0]
                proxy_config = {
                    'host': proxy.host,
                    'http_port': proxy.http_port,
                    'login': proxy.login,
                    'password': proxy.password
                }
            
            return await fetch_with_playwright(url, proxy_config=proxy_config, timeout=15000)
        except Exception as e:
            logger.error(f"Playwright fallback –ø–æ–º–∏–ª–∫–∞: {e}")
            return None, f"Playwright error: {str(e)[:100]}"
    
    async def fetch_website(self, url: str, use_proxy: bool = True) -> Tuple[Optional[str], Optional[str]]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∑ –≤–∫–∞–∑–∞–Ω–æ–≥–æ URL
        
        Args:
            url: URL —Å–∞–π—Ç—É –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É
            use_proxy: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø—Ä–æ–∫—Å—ñ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º True)
        
        Returns:
            Tuple[html_content, error_message]
            - html_content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∞–±–æ None –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ
            - error_message: –ü–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –ø–æ–º–∏–ª–∫—É –∞–±–æ None –ø—Ä–∏ —É—Å–ø—ñ—Ö—É
        """
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ URL
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Å–µ—Å—ñ—é –æ–¥–∏–Ω —Ä–∞–∑
        session = await self._get_session(use_proxy=use_proxy and self.proxy_rotator is not None)
        
        for attempt in range(self.max_retries):
            proxy_base_url = None
            proxy_auth = None

            try:
                # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ–∫—Å—ñ
                if use_proxy and self.proxy_rotator:
                    parts = self.proxy_rotator.get_next_proxy_for_aiohttp(proxy_type="http")
                    if not parts:
                        logger.error("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø—Ä–æ–∫—Å—ñ")
                        return None, "–í—Å—ñ –ø—Ä–æ–∫—Å—ñ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ"
                    proxy_base_url, login, password = parts
                    proxy_auth = aiohttp.BasicAuth(login, password) if (login and password) else None

                headers = self._get_headers(url)
                kwargs = {'headers': headers}
                if proxy_base_url:
                    kwargs['proxy'] = proxy_base_url
                    if proxy_auth:
                        kwargs['proxy_auth'] = proxy_auth

                logger.info(
                    f"–°–ø—Ä–æ–±–∞ {attempt + 1}/{self.max_retries}: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {url}" +
                    (f" —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å—ñ {proxy_base_url}" if proxy_base_url else "")
                )

                async with session.get(url, **kwargs) as response:
                        if response.status == 200:
                            html_content = await response.text()
                            
                            # –£—Å–ø—ñ—à–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è - –≤—ñ–¥–º—ñ—á–∞—î–º–æ –ø—Ä–æ–∫—Å—ñ —è–∫ —Ä–æ–±–æ—á–∏–π
                            if proxy_base_url and self.proxy_rotator:
                                self.proxy_rotator.mark_proxy_success(proxy_base_url)
                            
                            logger.info(f"‚úì –£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {url} ({len(html_content)} –±–∞–π—Ç)")
                            return html_content, None
                        
                        else:
                            error_msg = f"HTTP {response.status}: {response.reason}"
                            logger.warning(f"‚úó {error_msg} –¥–ª—è {url}")
                            
                            # 403 - –∞–Ω—Ç–∏–±–æ—Ç –∑–∞—Ö–∏—Å—Ç, –ø—Ä–æ–±—É—î–º–æ Playwright
                            if response.status == 403:
                                logger.info(f"üåê –ü—Ä–æ–±—É—î–º–æ Playwright –¥–ª—è {url} (–∞–Ω—Ç–∏–±–æ—Ç 403)")
                                playwright_html, playwright_error = await self._try_playwright(url)
                                if playwright_html:
                                    return playwright_html, None
                                else:
                                    logger.warning(f"Playwright —Ç–µ–∂ –Ω–µ –∑–º—ñ–≥: {playwright_error}")
                                    return None, f"403 + Playwright failed: {playwright_error}"
                            
                            # 429 - rate limit, –ø–æ–≤—Ç–æ—Ä—é—î–º–æ
                            if response.status == 429:
                                if proxy_base_url and self.proxy_rotator:
                                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)
                                # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ retry loop
                            # –Ü–Ω—à—ñ 4xx –ø–æ–º–∏–ª–∫–∏ - –Ω–µ –ø–æ–≤—Ç–æ—Ä—é—î–º–æ
                            elif 400 <= response.status < 500:
                                return None, error_msg
                            else:
                                # 5xx –ø–æ–º–∏–ª–∫–∏ - –ø–æ–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–æ–∫—Å—ñ —è–∫ –Ω–µ–≤–¥–∞–ª–∏–π
                                if proxy_base_url and self.proxy_rotator:
                                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)

            except asyncio.TimeoutError:
                error_msg = f"Timeout –ø—ñ—Å–ª—è {settings.SCRAPING_TIMEOUT} —Å–µ–∫—É–Ω–¥"
                logger.warning(f"‚úó {error_msg} –¥–ª—è {url}")
                if proxy_base_url and self.proxy_rotator:
                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)

            except aiohttp.ClientError as e:
                error_msg = f"–ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è ({type(e).__name__}): {str(e)}"
                logger.warning(f"‚úó {error_msg} –¥–ª—è {url}" + (f" (–ø—Ä–æ–∫—Å—ñ {proxy_base_url})" if proxy_base_url else ""))
                if proxy_base_url and self.proxy_rotator:
                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)

            except Exception as e:
                error_msg = f"–ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {str(e)}"
                logger.error(f"‚úó {error_msg} –¥–ª—è {url}", exc_info=True)
                if proxy_base_url and self.proxy_rotator:
                    self.proxy_rotator.mark_proxy_failed(proxy_base_url)
            
            # –ß–µ–∫–∞—î–º–æ –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–æ—é —Å–ø—Ä–æ–±–æ—é (exponential backoff –∑ jitter)
            if attempt < self.max_retries - 1:
                base_wait = min(BACKOFF_BASE ** attempt, BACKOFF_MAX)
                jitter = random.uniform(0, base_wait * BACKOFF_JITTER)
                wait_time = base_wait + jitter
                logger.info(f"–ß–µ–∫–∞—î–º–æ {wait_time:.1f}—Å –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–æ—é —Å–ø—Ä–æ–±–æ—é...")
                await asyncio.sleep(wait_time)
        
        return None, f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ø—ñ—Å–ª—è {self.max_retries} —Å–ø—Ä–æ–±"
    
    def extract_visible_content(self, html: str, base_url: str) -> Dict[str, Any]:
        """
        –í–∏—Ç—è–≥–Ω—É—Ç–∏ –≤–∏–¥–∏–º–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∑ HTML
        
        Args:
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç
            base_url: –ë–∞–∑–æ–≤–∏–π URL –¥–ª—è —Ä–µ–∑–æ–ª—é—Ü—ñ—ó –≤—ñ–¥–Ω–æ—Å–Ω–∏—Ö –ø–æ—Å–∏–ª–∞–Ω—å
        
        Returns:
            Dict –∑ –≤–∏—Ç—è–≥–Ω—É—Ç–∏–º–∏ –¥–∞–Ω–∏–º–∏:
            - title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            - text: –í–∏–¥–∏–º–∏–π —Ç–µ–∫—Å—Ç
            - links: –°–ø–∏—Å–æ–∫ –ø–æ—Å–∏–ª–∞–Ω—å
            - meta_description: Meta –æ–ø–∏—Å
            - clean_html: –û—á–∏—â–µ–Ω–∏–π HTML (–±–µ–∑ scripts, styles)
        """
        soup = BeautifulSoup(html, 'lxml')
        
        # –í–∏–¥–∞–ª—è—î–º–æ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ç–µ–≥–∏
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe', 'noscript']):
            tag.decompose()
        
        # –í–∏—Ç—è–≥—É—î–º–æ title
        title = soup.title.string if soup.title else ""
        
        # –í–∏—Ç—è–≥—É—î–º–æ meta description
        meta_desc = ""
        meta_tag = soup.find('meta', attrs={'name': 'description'})
        if meta_tag and meta_tag.get('content'):
            meta_desc = meta_tag.get('content')
        
        # –í–∏—Ç—è–≥—É—î–º–æ –≤–∏–¥–∏–º–∏–π —Ç–µ–∫—Å—Ç
        text = soup.get_text(separator=' ', strip=True)
        # –û—á–∏—â–∞—î–º–æ –º–Ω–æ–∂–∏–Ω–Ω—ñ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –í–∏—Ç—è–≥—É—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
        links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            # –†–µ–∑–æ–ª–≤–∏–º–æ –≤—ñ–¥–Ω–æ—Å–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
            absolute_url = urljoin(base_url, href)
            link_text = link.get_text(strip=True)
            if link_text:
                links.append({
                    'url': absolute_url,
                    'text': link_text
                })
        
        # –û—á–∏—â–µ–Ω–∏–π HTML (–¥–ª—è Gemini)
        clean_html = str(soup)
        
        return {
            'title': title.strip() if title else "",
            'text': text[:MAX_TEXT_LENGTH],
            'links': links[:MAX_LINKS_COUNT],
            'meta_description': meta_desc.strip() if meta_desc else "",
            'clean_html': clean_html[:MAX_HTML_LENGTH]
        }
    
    async def scrape_domain(self, domain: str, use_proxy: bool = True, use_cache: bool = True) -> Dict[str, Any]:
        """
        –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥—É –¥–æ–º–µ–Ω—É –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –∫–µ—à—É–≤–∞–Ω–Ω—è
        
        Args:
            domain: –î–æ–º–µ–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É (–º–æ–∂–Ω–∞ –∑ –∞–±–æ –±–µ–∑ https://)
            use_proxy: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø—Ä–æ–∫—Å—ñ
            use_cache: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Redis –∫–µ—à (TTL: 1 –≥–æ–¥–∏–Ω–∞)
        
        Returns:
            Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            - success: bool - —á–∏ —É—Å–ø—ñ—à–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥
            - domain: str - –¥–æ–º–µ–Ω
            - url: str - –ø–æ–≤–Ω–∏–π URL
            - html_raw: str - —Å–∏—Ä–∏–π HTML (–º–æ–∂–µ –±—É—Ç–∏ None)
            - content: dict - –≤–∏—Ç—è–≥–Ω—É—Ç–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (–º–æ–∂–µ –±—É—Ç–∏ None)
            - error: str - –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –ø–æ–º–∏–ª–∫—É (–º–æ–∂–µ –±—É—Ç–∏ None)
            - cached: bool - —á–∏ –æ—Ç—Ä–∏–º–∞–Ω–æ –∑ –∫–µ—à—É
        """
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –¥–æ–º–µ–Ω
        if not domain.startswith(('http://', 'https://')):
            url = 'https://' + domain
        else:
            url = domain
            domain = urlparse(url).netloc
        
        result = {
            'success': False,
            'domain': domain,
            'url': url,
            'html_raw': None,
            'content': None,
            'error': None,
            'cached': False
        }
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –∫–µ—à –æ–¥–∏–Ω —Ä–∞–∑
        cache = None
        if use_cache:
            try:
                cache = await get_cache()
            except Exception as e:
                logger.warning(f"–ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –∫–µ—à—É: {e}")
        
        # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∑ –∫–µ—à—É
        if cache:
            try:
                cached_data = await cache.get_html(domain)
                if cached_data:
                    result['success'] = True
                    result['html_raw'] = cached_data.get('html_raw')
                    result['content'] = cached_data.get('content')
                    result['cached'] = True
                    logger.info(f"‚úì –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –∫–µ—à –¥–ª—è {domain}")
                    return result
            except Exception as e:
                logger.warning(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∫–µ—à—É: {e}")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ HTML
        html, error = await self.fetch_website(url, use_proxy=use_proxy)
        
        if html:
            result['success'] = True
            result['html_raw'] = html
            result['content'] = self.extract_visible_content(html, url)
            
            # –ó–±–µ—Ä–µ–≥—Ç–∏ –≤ –∫–µ—à
            if cache:
                try:
                    await cache.set_html(domain, {
                        'html_raw': html,
                        'content': result['content']
                    })
                except Exception as e:
                    logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É –≤ –∫–µ—à: {e}")
        else:
            result['error'] = error
        
        return result
    
    @classmethod
    def create_with_config(cls, proxy_config: Optional[Dict] = None) -> "WebScraper":
        """
        –°—Ç–≤–æ—Ä–∏—Ç–∏ WebScraper –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é –ø—Ä–æ–∫—Å—ñ
        
        Args:
            proxy_config: Dict –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏ –ø—Ä–æ–∫—Å—ñ –∞–±–æ None
        
        Returns:
            WebScraper instance
        """
        proxy_rotator = None
        
        if proxy_config:
            proxy_rotator = ProxyRotator.from_config(proxy_config)
        
        return cls(proxy_rotator=proxy_rotator)
